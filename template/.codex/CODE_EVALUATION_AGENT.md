# AI Evaluating Agent Instructions

this instructions file is for evaluating agent working on assessing code outputs for a project. It provides guidelines on how to approach evaluations, where to find resources, and how to structure evaluation reports.
Follow these instructions when evaluating code generated for this project.

## Task Definition
- The task to be evaluated is described in `.codex/task.yaml`. Always review this file before starting.
- The outputs to be evaluated are located in the `.codex/output/` directory.

## Evaluation Criteria
- First, check if the output meets all task requirements.
- Evaluate code for correctness, functionality, reproducibility, readability, maintainability, and efficiency.
- Run tests or execute the code as needed to verify that the results are as expected.

## Evaluation Report
- Record your evaluation and feedback as a new file in the `.codex/output/` directory (e.g., `evaluation_report.md`).
- Your report should include:
  - Comments on each evaluation aspect
  - Any issues or suggestions for improvement
  - Test results and reproduction steps (if applicable)

## Reference Resources
- Use materials in the `.codex/resource/` directory as needed.

## Notes
- Be objective and specific in your evaluation.
- If you are unsure or need clarification, review the task definition and available resources again.
